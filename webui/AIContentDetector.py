'''
Copyright 2024 Zhixuan Hu

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
'''

import streamlit as st
from streamlit_modal import Modal

import sys, os, re, math, importlib
from string import Template
import jieba.analyse

os.chdir(os.path.dirname(__file__))
sys.path.append(os.path.join(os.getcwd(), '../'))
from settings import *
from llm_models.gpt_3_5_turbo_0125 import get_chat_response
supported_models = [model for model in os.listdir('../main_models') if os.path.isdir(os.path.join('../main_models', model))]
specialized_models = [(model_name, importlib.import_module(f"specialized_models.{model_name}.inference").Model()) for model_name in [
    'emotion_Dou',
    'sentences_length',
    'words_richness'
]]

## åŠ è½½æ¨¡å‹ï¼Œå¯¼å…¥æ¨¡å‹å¯¹åº”çš„inferenceæ¨¡å—
@st.cache_resource
def init_main_model(model_name):
    pths = os.listdir(os.path.join('../main_models', model_name, 'pths'))
    inference = importlib.import_module(f"main_models.{model_name}.inference")
    return pths, inference

## è§„èŒƒå­—ç¬¦ä¸²åˆ°è¾¹æ ä¼šè¯æŒ‰é’®è¦æ±‚
@st.cache_data
def adj_str(input_str, length=18, padding_char='\u3000'):
    actual_length = sum(2 if ord(char) > 127 else 1 for char in input_str)
    if actual_length > length:
        return input_str[:length]
    else:
        return input_str.ljust(length - (actual_length - len(input_str)), padding_char)

## åˆ†å‰²æ–‡æœ¬ä¸ºè¯­æ®µ
@st.cache_data
def split_text_into_segments(text, tol=200, reg=r'[\n]'):
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²æ–‡æœ¬ï¼Œåˆ†éš”ç¬¦ä¸ºå¥å·ã€é—®å·æˆ–æ„Ÿå¹å·
    segments = re.split(reg, text)
    # è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²ï¼Œå¹¶åœ¨æ¯ä¸ªæ®µè½ååŠ ä¸ŠåŸæ¥çš„ç»“æŸç¬¦å·
    segments = [seg.strip() for seg in segments if seg.strip() != '']
     # åˆå¹¶é•¿åº¦å°äº tol çš„è¯­æ®µ
    merged_segments = []
    current_segment = ''
    
    for seg in segments:
        if len(current_segment) + len(seg) + 1 <= tol:
            current_segment += (' ' + seg if current_segment else seg)
        else:
            if current_segment:
                merged_segments.append(current_segment)
            current_segment = seg

    if current_segment:
        merged_segments.append(current_segment)
    
    return merged_segments

## è¡¥å…¨é—®é¢˜æ–‡æœ¬
def generate_question(text, num_topics):

    stopwords = {
        'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'åœ¨', 'ä¹Ÿ', 'æœ‰', 'å°±', 'ä¸', 'éƒ½', 'è€Œ', 'åŠ', 'ä¸', 'æˆ–', 'ä¸€ä¸ª', 'ä¸­', 'è¿™', 'ä»¥åŠ', 'ä¸€ä¸ª',
        'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'å¥¹ä»¬', 'å®ƒä»¬', 'å…¶', 'å°†', 'è¦', 'å·²ç»', 'è¿˜', 'è¿˜è¦', 'å†',
        'æ²¡æœ‰', 'ä¸æ˜¯', 'éå¸¸', 'ç‰¹åˆ«', 'å¾ˆ', 'ç‰¹åˆ«', 'å¤ª', 'ç„¶å', 'ä½†æ˜¯', 'æ‰€ä»¥', 'å¦‚æœ', 'å› ä¸º', 'å› ä¸ºè€Œ', 'æ‰€ä»¥æ‰', 'ç”±äº', 'å› æ­¤', 
        'å°±', 'è€Œä¸”', 'ç„¶è€Œ', 'å¹¶ä¸”', 'å°±æ˜¯', 'å³', 'åˆ', 'è¿™', 'é‚£', 'è¿™äº›', 'é‚£äº›', 'æ—¶å€™', 'å½“æ—¶', 'å·²', 'æˆ‘', 'ä½ ', 'ä»–',
        'å¥¹', 'å®ƒ', 'çš„', 'ä¸€ä¸ª', 'å’Œ', 'è€Œä¸”', 'å¹¶', 'ä¸', 'æ¥', 'åœ¨', 'åˆ°', 'ä¹Ÿ', 'æœ‰', 'è‡ªå·±', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬',
        'å¥¹ä»¬', 'å®ƒä»¬', 'è¯¥', 'ä»¥åŠ', 'ä½†', 'å¹¶', 'åˆ', 'åŒ', 'æ¥ç€', 'ç­‰', 'å¯æ˜¯', 'è€Œ', 'äºæ˜¯', 'è€Œæ˜¯', 'å¹¶ä¸”', 'ä»¥åŠ', 
        'è¿˜', 'è¿˜æ˜¯', 'ä»ç„¶', 'è¿˜æ˜¯', 'ç”šè‡³', 'çš„', 'äº†', 'å°±', 'éƒ½', 'è€Œ', 'å’Œ', 'ä¸', 'è¿™', 'ä¹Ÿ', 'ä¸€', 'æ¥', 'å°±', 'ä¸',
        'ä¸­', 'ä¸º', 'åœ¨', 'å¯', 'å´', 'ä¸', 'äº', 'ä»–', 'å¥¹', 'å®ƒ', 'èƒ½', 'æ‰€', 'ä¸ª', 'äºº', 'è¿™', 'é‚£', 'ä¹Ÿ', 'è€Œ', 'ä½†', 
        'æŠŠ', 'å´', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'å¥¹ä»¬', 'å®ƒä»¬', 'è¿™', 'é‚£', 'è¿™äº›', 'é‚£äº›'
    }

    kw = jieba.analyse.extract_tags(text, topK=num_topics)

    # # è·å–æ–‡ç« å¼€å¤´çš„2-4å¥è¯
    # sentences = split_sentences(text)
    # start_sentences = ' '.join(sentences[:random.randint(2, 4)])

    # æ„å»ºæ–°çš„é—®é¢˜æ–‡æœ¬
    # new_question = f"è¯·æ ¹æ®å…³é”®è¯{kw}å’Œä»¥{start_sentences}ä¸ºå¼€å¤´çš„è¿›è¡Œå†™ä½œã€‚"
    new_question = f"è¯·æ ¹æ®å…³é”®è¯{kw}è¿›è¡Œå†™ä½œã€‚"

    return new_question

## åˆ é™¤ä¼šè¯
def delete_session(id):
    def delete():
        del st.session_state.history[id]
    # å¼¹å‡ºç¡®è®¤å¯¹è¯æ¡†
    session = st.session_state.history[id]
    msg = Modal(title="#### åˆ é™¤ä¼šè¯", key="delete", max_width=500).container()
    with msg:
        st.markdown(f"ç¡®è®¤åˆ é™¤è¯¥ç”¨ä¾‹ï¼Ÿ \n{session['name']}")
        b1, b2 = st.columns(spec=[1, 6])
        b1.button("ç¡®è®¤", on_click=delete) # ä½¿ç”¨onlickæ‰ä¼šè°ƒç”¨æˆåŠŸå¹¶æˆåŠŸåˆ·æ–°ï¼Œä¸èƒ½ç”¨if
        b2.button("å–æ¶ˆ")

## é‡å‘½åä¼šè¯
def rename_session(id):
    def tru_rename():
        st.session_state.history[id]['name'] = st.session_state['text']
    # å¼¹å‡ºç¡®è®¤å¯¹è¯æ¡†
    def rename():
        st.session_state.text = st.session_state['rename-text'] # è¯»å–è‡ªåŠ¨ä¿å­˜çš„è¾“å…¥æ–‡æœ¬ï¼Œç›®å‰åªèƒ½é‡‡ç”¨æ­¤ç­–ç•¥è¯»åˆ°æ–‡æœ¬
        msg = Modal(title="#### ç¡®è®¤é‡å‘½å", key="confirm-rename", max_width=500).container()
        with msg:
            st.markdown(f"ç¡®è®¤é‡å‘½å  \n`{oldname}`  \nä¸ºï¼š  \n`{st.session_state['text']}`ï¼Ÿ")
            b1, b2 = st.columns(spec=[1, 6])
            b1.button("ç¡®è®¤", on_click=tru_rename)
            b2.button("å–æ¶ˆ")
    # å¼¹å‡ºè¾“å…¥å¯¹è¯æ¡†
    session = st.session_state.history[id]
    oldname = session['name']
    msg = Modal(title="#### é‡å‘½åä¼šè¯", key="rename", max_width=500).container()
    with msg:
        st.text_input(label=" ", key="rename-text", value=session['name'], on_change=rename) # å½“æŒ‰ä¸‹å›è½¦ä¼šè§¦å‘on_change

## æ–°å»ºä¼šè¯
def new_session():
    def create():
        st.session_state.history.append({
            "id": len(st.session_state.history),   # å”¯ä¸€æ ‡è¯†
            'name': st.session_state['new-text'],  # ä¼šè¯åç§°
            "tab_count": 2,                        # tabæ•°é‡
            'data': [{                             # å„ä¸ªtabçš„ä¿å­˜ä¿¡æ¯
                    "model_index": DEFAULT_MODEL_INDEX,        # é€‰è¿‡çš„æ¨¡å‹id
                    "device_index": DEFAULT_DEVICE_INDEX,       # é€‰è¿‡çš„è®¾å¤‡id
                    "pth_index": DEFAULT_PTH_INDEX,          # é€‰è¿‡çš„å‚æ•°id
                }, {
                    "model_index": DEFAULT_MODEL_INDEX,
                    "device_index": DEFAULT_DEVICE_INDEX,
                    "pth_index": DEFAULT_PTH_INDEX,
                },
            ]
        })
    # å¼¹å‡ºè¾“å…¥å¯¹è¯æ¡†
    msg = Modal(title="#### æ–°å»ºä¼šè¯", key="new", max_width=500).container()
    with msg:
        st.text_input(label=" ", key="new-text", value="ä¼šè¯ ", on_change=create)

## é€šè¿‡onchangeè§¦å‘ï¼Œç¼“å­˜é—®é¢˜æ–‡æœ¬
def save_question_text(data, key):
    data['question_text'] = st.session_state[key]

## é€šè¿‡onchangeè§¦å‘ï¼Œç¼“å­˜å›ç­”æ–‡æœ¬
def save_answer_text(data, key):
    data['answer_text'] = st.session_state[key]

class Reporter():
    def __init__(self):
        self.local_templates = {
            'default': Template("""
                <div style="background-color: ${color}; padding: 10px; margin: 10px 0;">
                    <strong>${num}. æ®µè½é¢„æµ‹æ¦‚ç‡: ${prob}%</strong>
                    <p>${text}</p>
                </div>
            """)}
        
        self.prompt_templates = {
            'default': Template("""
                ${num}. æ®µè½é¢„æµ‹æ¦‚ç‡: ${prob}%
            """)}
        
        self.global_templates = {
            'default': Template("""
                ### ä¸»æ¨¡å‹ç»¼åˆé¢„æµ‹æ¦‚ç‡  
                    ${main}ï¼›
                ### ç‰¹åŒ–æ¨¡å‹é¢„æµ‹æ¦‚ç‡ï¼š  
                - è¯è¯­ä¸°å¯Œåº¦æ¨¡å‹ï¼š
                    ${words_richness}ï¼›
                - å¥å­é•¿åº¦æ¨¡å‹ï¼š
                    ${sentences_length}ï¼›
                - æƒ…æ„Ÿå¼ºåº¦æ¨¡å‹ï¼š
                    ${emotion_Dou}ï¼›                
            """)}

    # æ ¹æ®æ¦‚ç‡è®¡ç®—é¢œè‰²çš„æ·±åº¦
    def color_gradient(self, prob):
        intensity = 255 - int(math.floor(155 * prob))  # ä¿è¯è‡³å°‘æ˜¯100ï¼Œæ‰€ä»¥é¢œè‰²ä¸ä¼šå¤ªæ·±
        return f"rgb(255, {intensity}, {intensity})"  # çº¢è‰²æ¸å˜
    
    # ç”Ÿæˆåˆ†æ®µé¢„æµ‹æŠ¥å‘Š
    def local_render(self, template, segment, probs):
        local_template = self.local_templates[template]
        ans_seg = [
            local_template.substitute(text=text, prob=f"{prob*100:.1f}", color=self.color_gradient(prob), num=num)
            for num, (text, prob) in enumerate(zip(segment, probs), start=1)
        ]
        return "\n".join(ans_seg)
    
    def local_prompt(self, template, probs):
        template = self.prompt_templates[template]
        ans = [
            template.substitute(prob=f"{prob*100:.1f}", num=num)
            for num, prob in enumerate(probs, start=1)
        ]
        return "\n".join(ans)

    # ç”Ÿæˆæ€»ä½“é¢„æµ‹æŠ¥å‘Š
    def global_render(self, template, probs):
        global_template = self.global_templates[template]
        return global_template.substitute(**probs)
    


























## åˆå§‹åŒ–ï¼Œåˆ›å»ºå†å²ä¼šè¯è®°å½•
if not hasattr(st.session_state, 'history'):
    st.session_state.history = []
if not hasattr(st.session_state, 'current'):
    st.session_state.current = {}

if 'reporter' not in globals():
    reporter = Reporter()

st.set_page_config(page_title="llmFlight", page_icon=' ', layout='wide')

## ä¾§è¾¹æ 
st.sidebar.header("å†å²ä¼šè¯")

for session in st.session_state.history:
    c1, c2, c3 = st.sidebar.columns(spec=[9, 2, 2])
    if c1.button(adj_str(session['name']), key=f"{session['id']}-on"):
        # ç‚¹å‡»ååŠ è½½sessionåˆ°current
        st.session_state.current = session
    c2.button("ğŸ—‘ï¸", key=f"{session['id']}-delete", on_click=delete_session, args=(session['id'],))
    c3.button("ğŸ–‰", key=f"{session['id']}-rename", on_click=rename_session, args=(session['id'],))

st.sidebar.button("ï¼‹ æ–°å»ºä¼šè¯", key="new_session", type="primary", on_click=new_session)

## æ ¹æ®currentåŠ è½½ä¸»ç•Œé¢
if st.session_state.current:
    session = st.session_state.current
    id = session['id']
    st.markdown(f"#### {session['name']}")
    # åŠ è½½å„ä¸ªtab
    for tabid, tab in enumerate(st.tabs([f"Tab{i+1}" for i in range(session['tab_count'])])):
        with tab:
            # é€‰æ‹©æ¨¡å‹ã€è®¾å¤‡ã€å‚æ•°
            data = session['data'][tabid]
            num_options = len(supported_models)
            model_name = st.selectbox(label="model", key=f'select-model-{id}-{tabid}', options=supported_models, index=(num_options+data['model_index']) % num_options)
            data['model_index'] = supported_models.index(model_name)
            pths, inference = init_main_model(model_name)
            supported_devices = st.cache_resource(inference.supported_devices)()
            num_options = len(supported_devices)
            device = st.selectbox(label="device", key=f'select-device-{id}-{tabid}', options=list(supported_devices.keys()), index=(num_options+data['device_index']) % num_options)
            data['device_index'] = list(supported_devices.keys()).index(device)
            num_options = len(pths)
            pth = st.selectbox(label="model params", key=f'select-params-{id}-{tabid}', options=pths, index=(num_options+data['pth_index']) % num_options)
            data['pth_index'] = pths.index(pth)
            model = inference.Model(os.path.join("../main_models", model_name, "pths", pth), supported_devices[device])

            # è¾“å…¥æ–‡æœ¬
            question_text = st.text_area(label="question", value=data.get('question_text', ""), height=100, key=f"question-input-{id}-{tabid}", 
                                         on_change=save_question_text, args=(data, f"question-input-{id}-{tabid}"))
            st.write(f"The text has about `{sum(2 if ord(char) > 127 else 1 for char in question_text)}` tokens.")
            answer_text = st.text_area(label="answer", value=data.get('answer_text', ""), height=500, key=f"answer-input-{id}-{tabid}",
                                       on_change=save_answer_text, args=(data, f"answer-input-{id}-{tabid}"))
            st.write(f"The text has about `{sum(2 if ord(char) > 127 else 1 for char in answer_text)}` tokens.")

            cc1, cc2 = st.columns(spec=[1, 9])
            # è¡¥å…¨é—®é¢˜æ–‡æœ¬
            if cc1.button("ç”Ÿæˆé—®é¢˜", key=f"generate-question-{id}-{tabid}"):
                question_text = generate_question(answer_text, 10)
                st.markdown(f"`{question_text}`")
                data['question_text'] = question_text

            # é¢„æµ‹
            if cc2.button("æ£€æµ‹", key=f"infer-{id}-{tabid}"):
                question_embedding = inference.get_embeddings([question_text])[0]
                answer_seg = [answer_text] + split_text_into_segments(answer_text)
                answer_embeddings = inference.get_embeddings(answer_seg)
                probs = [model.infer(question_embedding, answer_embedding) for answer_embedding in answer_embeddings]
                specialized_probs = {**{'main': f"{probs[0]*100:.1f}%"}, **{model_name: f"{model.infer(answer_text)[0]*100:.1f}%" for model_name, model in specialized_models}}

                global_info = reporter.global_render('default', specialized_probs)
                st.markdown(global_info)
                st.markdown("### Agentæ€»ç»“ï¼š")
                prompt = """
                    ä»¥ä¸‹æ˜¯AIæ–‡æœ¬æ£€æµ‹æŠ¥å‘Šï¼Œä½†ä¸å¤Ÿç›´è§‚ï¼Œè¯·å¸®æˆ‘æ±‡æ€»ä¸€ä¸ªç®€æ´çš„ç»“è®ºå‡ºæ¥ã€‚å¦‚æœç»¼åˆæ¦‚ç‡é«˜ï¼Œè¯·è¿›è¡Œå½’å› ã€‚ä»¥è¿™æ ·çš„æ ¼å¼ï¼š
                    ã€ç®€æ´ç»“è®ºã€‘\n
                    â€¦â€¦\n
                    ã€å…·ä½“åˆ†æã€‘\n
                    è¯è¯­ä¸°å¯Œåº¦æ¨¡å‹é¢„æµ‹æ–‡æœ¬ä¸ºAIç”Ÿæˆçš„æ¦‚ç‡â€¦â€¦ï¼Œè¡¨æ˜â€¦â€¦
                    å¥å­é•¿åº¦æ¨¡å‹å’Œæƒ…æ„Ÿå¼ºåº¦æ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡åˆ†åˆ«ä¸ºâ€¦â€¦â€¦â€¦
                    åˆ†æ®µé¢„æµ‹ä¸­ï¼Œâ€¦â€¦çš„é¢„æµ‹æ¦‚ç‡è¾ƒé«˜ï¼Œåˆ†åˆ«ä¸ºâ€¦â€¦ï¼Œæç¤ºâ€¦â€¦\n
                    ç»¼åˆæ¥çœ‹ï¼Œâ€¦â€¦
                """ + global_info + reporter.local_prompt('default', probs[1:])
                st.write(get_chat_response(prompt)[0])
                st.markdown("### åˆ†æ®µé¢„æµ‹æ¦‚ç‡ï¼š")
                st.html(reporter.local_render('default', answer_seg[1:], probs[1:]))
                

































































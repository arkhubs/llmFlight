{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2024 Zhixuan Hu\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, re, sys, json, time\n",
    "sys.path.append(os.getcwd() + '\\\\..\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chatgpt_HC3_all.csv',\n",
       " 'chatgpt_HC3_all.csv-2024-06-04-21-04-24.csv',\n",
       " 'chatgpt_HC3_all.csv-2024-06-05-12-05-06.csv',\n",
       " 'chatgpt_HC3_all.csv-2024-06-05-12-39-20.csv',\n",
       " 'gpt_0125_00.csv',\n",
       " 'human_00.csv',\n",
       " 'human_01_01.csv',\n",
       " 'human_01_02.csv',\n",
       " 'human_02.csv',\n",
       " 'human_cook_01.csv',\n",
       " 'human_cook_02.csv',\n",
       " 'human_HC3_all.csv',\n",
       " 'human_HC3_all.csv-2024-06-04-21-02-55.csv',\n",
       " 'human_HC3_all.csv-2024-06-05-11-36-01.csv',\n",
       " 'human_modern_01.csv',\n",
       " 'human_modern_02.csv',\n",
       " 'human_travel_01.csv',\n",
       " 'human_travel_02.csv',\n",
       " 'human_travel_03.csv',\n",
       " 'questions_00.csv',\n",
       " 'questions_01_01.csv',\n",
       " 'questions_01_02.csv',\n",
       " 'questions_02.csv',\n",
       " 'questions_cook_01.csv',\n",
       " 'questions_cook_02.csv',\n",
       " 'questions_HC3_all.csv',\n",
       " 'questions_modern_01.csv',\n",
       " 'questions_modern_02.csv',\n",
       " 'questions_travel_01.csv',\n",
       " 'questions_travel_02.csv',\n",
       " 'questions_travel_03.csv',\n",
       " 'test_gpt_0125.csv',\n",
       " 'test_gpt_0125.csv-2024-06-04-21-05-58.csv',\n",
       " 'test_gpt_0125.csv-2024-06-05-10-49-57.csv',\n",
       " 'test_human.csv',\n",
       " 'test_human.csv-2024-06-04-21-05-32.csv',\n",
       " 'test_human.csv-2024-06-05-11-05-02.csv',\n",
       " 'test_questions.csv']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(r\".*\\.csv\")\n",
    "files = os.listdir('../../dataset/')\n",
    "files = [file for file in files if pattern.match(file)]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.5</th>\n",
       "      <th>Unnamed: 0.4</th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>text</th>\n",
       "      <th>text-embedding-3-small</th>\n",
       "      <th>sentences_length[mean1,cv1,mean2,cv2]</th>\n",
       "      <th>words_richness[ttr, yules_k, d_index]</th>\n",
       "      <th>emotion_Dou[eps, eiv]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>454</td>\n",
       "      <td>1</td>\n",
       "      <td>深度学习是一种机器学习的方法，它通过模拟人类神经网络的结构和功能来实现对数据的学习和预测。在...</td>\n",
       "      <td>[-0.03873550891876221, -0.03209991380572319, 0...</td>\n",
       "      <td>[41.611111111111114, 0.25685931847790017, 14.3...</td>\n",
       "      <td>[0.33986928104575165, 235.04729899706192, 0.08...</td>\n",
       "      <td>[-0.99242573436361, 0.013853045618768956]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.5  Unnamed: 0.4  Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  \\\n",
       "0             0             0             0             0             0   \n",
       "\n",
       "   Unnamed: 0  question_id  answer_id  \\\n",
       "0         0.0          454          1   \n",
       "\n",
       "                                                text  \\\n",
       "0  深度学习是一种机器学习的方法，它通过模拟人类神经网络的结构和功能来实现对数据的学习和预测。在...   \n",
       "\n",
       "                              text-embedding-3-small  \\\n",
       "0  [-0.03873550891876221, -0.03209991380572319, 0...   \n",
       "\n",
       "               sentences_length[mean1,cv1,mean2,cv2]  \\\n",
       "0  [41.611111111111114, 0.25685931847790017, 14.3...   \n",
       "\n",
       "               words_richness[ttr, yules_k, d_index]  \\\n",
       "0  [0.33986928104575165, 235.04729899706192, 0.08...   \n",
       "\n",
       "                       emotion_Dou[eps, eiv]  \n",
       "0  [-0.99242573436361, 0.013853045618768956]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '../../dataset/test_gpt_0125.csv'\n",
    "df = pd.read_csv(file)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'emotion_Dou[eps, eiv]'\n",
    "# 检查是否存在 'as' 列\n",
    "if model not in df.columns:\n",
    "    # 如果不存在，则创建一个空的字符串列\n",
    "    df[model] = ''\n",
    "df[model] = df[model].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.5</th>\n",
       "      <th>Unnamed: 0.4</th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>text</th>\n",
       "      <th>text-embedding-3-small</th>\n",
       "      <th>sentences_length[mean1,cv1,mean2,cv2]</th>\n",
       "      <th>words_richness[ttr, yules_k, d_index]</th>\n",
       "      <th>emotion_Dou[eps, eiv]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>454</td>\n",
       "      <td>1</td>\n",
       "      <td>深度学习是一种机器学习的方法，它通过模拟人类神经网络的结构和功能来实现对数据的学习和预测。在...</td>\n",
       "      <td>[-0.03873550891876221, -0.03209991380572319, 0...</td>\n",
       "      <td>[41.611111111111114, 0.25685931847790017, 14.3...</td>\n",
       "      <td>[0.33986928104575165, 235.04729899706192, 0.08...</td>\n",
       "      <td>[-0.99242573436361, 0.013853045618768956]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.5  Unnamed: 0.4  Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  \\\n",
       "0             0             0             0             0             0   \n",
       "\n",
       "   Unnamed: 0  question_id  answer_id  \\\n",
       "0         0.0          454          1   \n",
       "\n",
       "                                                text  \\\n",
       "0  深度学习是一种机器学习的方法，它通过模拟人类神经网络的结构和功能来实现对数据的学习和预测。在...   \n",
       "\n",
       "                              text-embedding-3-small  \\\n",
       "0  [-0.03873550891876221, -0.03209991380572319, 0...   \n",
       "\n",
       "               sentences_length[mean1,cv1,mean2,cv2]  \\\n",
       "0  [41.611111111111114, 0.25685931847790017, 14.3...   \n",
       "\n",
       "               words_richness[ttr, yules_k, d_index]  \\\n",
       "0  [0.33986928104575165, 235.04729899706192, 0.08...   \n",
       "\n",
       "                       emotion_Dou[eps, eiv]  \n",
       "0  [-0.99242573436361, 0.013853045618768956]  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_need = df[('\"nan\"' == df[model]) | (df[model]=='')]\n",
    "# 防止过长超出token限制\n",
    "tol = 1e20\n",
    "mask = df_need['text'].str.len() > tol\n",
    "df_need.loc[mask, 'text'] = df_need.loc[mask, 'text'].str.slice(0, tol) #截断\n",
    "df_need_list = df_need['text'].to_list()\n",
    "df_need.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单进程\n",
    "# from emotion_Dou import main\n",
    "# df_need[model] = df_need['text'].apply(lambda row: json.dumps(main(row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多进程\n",
    "import json\n",
    "from emotion_Dou import main\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process_row(row):\n",
    "    return json.dumps(main(row))\n",
    "\n",
    "# 使用 joblib 进行并行计算\n",
    "results = Parallel(n_jobs=-1)(delayed(process_row)(row) for row in df_need['text'])\n",
    "\n",
    "# 将计算结果赋值回 DataFrame\n",
    "df_need[model] = results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text-embedding-3-small</th>\n",
       "      <th>sentences_length[mean1,cv1,mean2,cv2]</th>\n",
       "      <th>words_richness[ttr, yules_k, d_index]</th>\n",
       "      <th>emotion_Dou[eps, eiv]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>深度学习是一种机器学习的方法，它通过模拟人类神经网络的结构和功能来实现对数据的学习和预测。在...</td>\n",
       "      <td>[-0.03873550891876221, -0.03209991380572319, 0...</td>\n",
       "      <td>[41.611111111111114, 0.25685931847790017, 14.3...</td>\n",
       "      <td>[0.33986928104575165, 235.04729899706192, 0.08...</td>\n",
       "      <td>[-0.99242573436361, 0.013853045618768956]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>深度学习作为一种机器学习方法，其本质是通过神经网络来实现对数据的学习和模式识别。尽管深度学习...</td>\n",
       "      <td>[-0.03281135484576225, -0.04714532196521759, 0...</td>\n",
       "      <td>[43.642857142857146, 0.3040535969520589, 16.36...</td>\n",
       "      <td>[0.4383561643835616, 172.49014824544943, 0.137...</td>\n",
       "      <td>[-0.9971031135266845, 0.003428978282686927]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>深度学习并没有脱离数学，相反，数学在深度学习中扮演着非常重要的角色。深度学习是建立在数学基础...</td>\n",
       "      <td>[-0.05262744054198265, -0.041652824729681015, ...</td>\n",
       "      <td>[47.2, 0.22277015979821888, 13.176470588235293...</td>\n",
       "      <td>[0.5179856115107914, 220.4854821179028, 0.2460...</td>\n",
       "      <td>[-0.9999971667189304, 2.2681937399698345e-06]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>深度学习是一门需要深厚理论基础和丰富实践经验的学科，纯靠造假是无法取得真正的科研成果和发表高...</td>\n",
       "      <td>[0.05532760173082352, -0.00803462602198124, 0....</td>\n",
       "      <td>[43.36363636363637, 0.28333703678015437, 15.26...</td>\n",
       "      <td>[0.5083056478405316, 160.0423836381497, 0.1719...</td>\n",
       "      <td>[-0.9096644141027803, 0.12414851826658714]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>不应该仅仅依靠造假来发表论文。虽然有些人可能会通过欺骗和伪造数据来发表论文，但这种行为是极其...</td>\n",
       "      <td>[0.07378417253494263, -0.026973411440849304, 0...</td>\n",
       "      <td>[34.5, 0.4222841241835787, 16.75, 0.2248734204...</td>\n",
       "      <td>[0.6547619047619048, 167.23356009070295, 0.415...</td>\n",
       "      <td>[-0.7869503595847821, 0.07258366418451918]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>鸿蒙系统是华为公司自行研发的操作系统，它与安卓系统有所不同，虽然两者都是基于Linux内核开...</td>\n",
       "      <td>[0.009668500162661076, 0.00952873844653368, 0....</td>\n",
       "      <td>[50.18181818181818, 0.3370928507134205, 14.638...</td>\n",
       "      <td>[0.4646153846153846, 150.53254437869822, 0.169...</td>\n",
       "      <td>[-0.7812304893657067, 0.07706697231436901]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>鸿蒙系统和安卓系统是两个不同的操作系统。鸿蒙系统是由华为公司自主研发的操作系统，旨在为物联网...</td>\n",
       "      <td>[-0.012859110720455647, 0.021412188187241554, ...</td>\n",
       "      <td>[35.333333333333336, 0.43621237817724123, 17.1...</td>\n",
       "      <td>[0.711864406779661, 143.63688595231255, 0.5412...</td>\n",
       "      <td>[-0.91433732588865, 0.08247039458465749]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>关于黑人需要混几代血才能接近白人肤色这个问题，首先我认为肤色并不是衡量一个人价值和身份的标准...</td>\n",
       "      <td>[0.06337092816829681, -0.013654569163918495, 0...</td>\n",
       "      <td>[36.1764705882353, 0.35554398817855076, 13.044...</td>\n",
       "      <td>[0.34146341463414637, 207.49553837001784, 0.10...</td>\n",
       "      <td>[-0.7865466427458667, 0.15192894312026786]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>种族和肤色并不代表一个人的性格和人性，每个人都有自己独特的特质和价值，不应该用肤色来衡量一个...</td>\n",
       "      <td>[0.05830851197242737, 0.0020467150025069714, 0...</td>\n",
       "      <td>[32.4, 0.38776046356887434, 10.928571428571429...</td>\n",
       "      <td>[0.5922330097087378, 201.71552455462344, 0.324...</td>\n",
       "      <td>[-0.6353338957089568, 0.02992310353455195]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>黑人需要混合多代不同种族的血统才能逐渐接近白人肤色，但这并不意味着黑人需要放弃或改变他们的种...</td>\n",
       "      <td>[0.04827204719185829, 0.01748865656554699, 0.0...</td>\n",
       "      <td>[39.333333333333336, 0.17409015749300114, 19.1...</td>\n",
       "      <td>[0.6712328767123288, 157.62807280915743, 0.464...</td>\n",
       "      <td>[-0.9898447708271082, 0.014347364137388394]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>324 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    深度学习是一种机器学习的方法，它通过模拟人类神经网络的结构和功能来实现对数据的学习和预测。在...   \n",
       "1    深度学习作为一种机器学习方法，其本质是通过神经网络来实现对数据的学习和模式识别。尽管深度学习...   \n",
       "2    深度学习并没有脱离数学，相反，数学在深度学习中扮演着非常重要的角色。深度学习是建立在数学基础...   \n",
       "3    深度学习是一门需要深厚理论基础和丰富实践经验的学科，纯靠造假是无法取得真正的科研成果和发表高...   \n",
       "4    不应该仅仅依靠造假来发表论文。虽然有些人可能会通过欺骗和伪造数据来发表论文，但这种行为是极其...   \n",
       "..                                                 ...   \n",
       "319  鸿蒙系统是华为公司自行研发的操作系统，它与安卓系统有所不同，虽然两者都是基于Linux内核开...   \n",
       "320  鸿蒙系统和安卓系统是两个不同的操作系统。鸿蒙系统是由华为公司自主研发的操作系统，旨在为物联网...   \n",
       "321  关于黑人需要混几代血才能接近白人肤色这个问题，首先我认为肤色并不是衡量一个人价值和身份的标准...   \n",
       "322  种族和肤色并不代表一个人的性格和人性，每个人都有自己独特的特质和价值，不应该用肤色来衡量一个...   \n",
       "323  黑人需要混合多代不同种族的血统才能逐渐接近白人肤色，但这并不意味着黑人需要放弃或改变他们的种...   \n",
       "\n",
       "                                text-embedding-3-small  \\\n",
       "0    [-0.03873550891876221, -0.03209991380572319, 0...   \n",
       "1    [-0.03281135484576225, -0.04714532196521759, 0...   \n",
       "2    [-0.05262744054198265, -0.041652824729681015, ...   \n",
       "3    [0.05532760173082352, -0.00803462602198124, 0....   \n",
       "4    [0.07378417253494263, -0.026973411440849304, 0...   \n",
       "..                                                 ...   \n",
       "319  [0.009668500162661076, 0.00952873844653368, 0....   \n",
       "320  [-0.012859110720455647, 0.021412188187241554, ...   \n",
       "321  [0.06337092816829681, -0.013654569163918495, 0...   \n",
       "322  [0.05830851197242737, 0.0020467150025069714, 0...   \n",
       "323  [0.04827204719185829, 0.01748865656554699, 0.0...   \n",
       "\n",
       "                 sentences_length[mean1,cv1,mean2,cv2]  \\\n",
       "0    [41.611111111111114, 0.25685931847790017, 14.3...   \n",
       "1    [43.642857142857146, 0.3040535969520589, 16.36...   \n",
       "2    [47.2, 0.22277015979821888, 13.176470588235293...   \n",
       "3    [43.36363636363637, 0.28333703678015437, 15.26...   \n",
       "4    [34.5, 0.4222841241835787, 16.75, 0.2248734204...   \n",
       "..                                                 ...   \n",
       "319  [50.18181818181818, 0.3370928507134205, 14.638...   \n",
       "320  [35.333333333333336, 0.43621237817724123, 17.1...   \n",
       "321  [36.1764705882353, 0.35554398817855076, 13.044...   \n",
       "322  [32.4, 0.38776046356887434, 10.928571428571429...   \n",
       "323  [39.333333333333336, 0.17409015749300114, 19.1...   \n",
       "\n",
       "                 words_richness[ttr, yules_k, d_index]  \\\n",
       "0    [0.33986928104575165, 235.04729899706192, 0.08...   \n",
       "1    [0.4383561643835616, 172.49014824544943, 0.137...   \n",
       "2    [0.5179856115107914, 220.4854821179028, 0.2460...   \n",
       "3    [0.5083056478405316, 160.0423836381497, 0.1719...   \n",
       "4    [0.6547619047619048, 167.23356009070295, 0.415...   \n",
       "..                                                 ...   \n",
       "319  [0.4646153846153846, 150.53254437869822, 0.169...   \n",
       "320  [0.711864406779661, 143.63688595231255, 0.5412...   \n",
       "321  [0.34146341463414637, 207.49553837001784, 0.10...   \n",
       "322  [0.5922330097087378, 201.71552455462344, 0.324...   \n",
       "323  [0.6712328767123288, 157.62807280915743, 0.464...   \n",
       "\n",
       "                             emotion_Dou[eps, eiv]  \n",
       "0        [-0.99242573436361, 0.013853045618768956]  \n",
       "1      [-0.9971031135266845, 0.003428978282686927]  \n",
       "2    [-0.9999971667189304, 2.2681937399698345e-06]  \n",
       "3       [-0.9096644141027803, 0.12414851826658714]  \n",
       "4       [-0.7869503595847821, 0.07258366418451918]  \n",
       "..                                             ...  \n",
       "319     [-0.7812304893657067, 0.07706697231436901]  \n",
       "320       [-0.91433732588865, 0.08247039458465749]  \n",
       "321     [-0.7865466427458667, 0.15192894312026786]  \n",
       "322     [-0.6353338957089568, 0.02992310353455195]  \n",
       "323    [-0.9898447708271082, 0.014347364137388394]  \n",
       "\n",
       "[324 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer = df_need.index\n",
    "df.loc[indexer] = df_need\n",
    "df[df.columns[8:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_str = time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime())\n",
    "os.rename(file, '../../dataset/history/' + file + '-' + time_str + '.csv')\n",
    "df.to_csv(file, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lf",
   "language": "python",
   "name": "lf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
